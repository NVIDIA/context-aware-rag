{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Keys\n",
    "\n",
    "Export NVIDIA API keys for authentication\n",
    "Get your API keys from: https://build.nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Services\n",
    "\n",
    "Start all required services for CA-RAG:\n",
    "1. Export your NVIDIA API key:\n",
    "   ```\n",
    "   export NVIDIA_API_KEY=xxxx\n",
    "   ```\n",
    "\n",
    "2. Start the services using docker-compose:\n",
    "   ```\n",
    "   make -C docker start_compose\n",
    "   ```\n",
    "\n",
    "This will start the ingestion service (default port 8001) and retrieval service (default port 8000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NVIDIA_BUILD_API_KEY=xxxx\n",
    "%env NVIDIA_API_KEY=xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install NV-Ingest\n",
    "\n",
    "Install NV-Ingest following the steps\n",
    "\n",
    "Create a new uv virtual environment with nv-ingest\n",
    "\n",
    "```\n",
    "uv venv --python 3.12 nvingest && \\\n",
    "  source nvingest/bin/activate && \\\n",
    "  uv pip install pypdfium2==4.30.1\n",
    "  uv pip install nv-ingest==25.6.2 nv-ingest-api==25.6.2 nv-ingest-client==25.6.3\n",
    "```\n",
    "For more details, see nvingest documentation [here](https://docs.nvidia.com/nemo/retriever/latest/extraction/quickstart-library-mode/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start NV-Ingest Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the nv-ingest client to process the pdf documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\n",
    "from nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import PipelineCreationSchema\n",
    "from nv_ingest_client.client import Ingestor, NvIngestClient\n",
    "from nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient\n",
    "from nv_ingest_client.util.process_json_files import ingest_json_results_to_blob\n",
    "# Start the pipeline subprocess for library mode\n",
    "config = PipelineCreationSchema()\n",
    "\n",
    "run_pipeline(config, block=False, disable_dynamic_scaling=True, run_in_subprocess=True)\n",
    "\n",
    "client = NvIngestClient(\n",
    "    message_client_allocator=SimpleClient,\n",
    "    message_client_port=7670,\n",
    "    message_client_hostname=\"localhost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = (\n",
    "    Ingestor(client=client)\n",
    "    .files(\"data/multimodal_test.pdf\")\n",
    "    .extract(\n",
    "        extract_text=True,\n",
    "        extract_tables=True,\n",
    "        extract_charts=True,\n",
    "        extract_images=False,\n",
    "        paddle_output_format=\"markdown\",\n",
    "        extract_infographics=True,\n",
    "        # extract_method=\"nemoretriever_parse\", #Slower, but maximally accurate, especially for PDFs with pages that are scanned images\n",
    "        text_depth=\"page\"\n",
    "    )\n",
    ")\n",
    "\n",
    "results, failures = ingestor.ingest(show_progress=True, return_failures=True)\n",
    "\n",
    "print(ingest_json_results_to_blob(results[0]))\n",
    "print(failures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest the documents in Context Aware RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Documents to Context-Aware RAG\n",
    "\n",
    "This section demonstrates how to add documents to CA-RAG using POST requests to the ingestion service. We'll upload the previously extracted text, tables and charts for data ingestion. Once ingestion is complete, the documents will be available for question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload text results to server\n",
    "import requests\n",
    "import json\n",
    "import pyaml_env\n",
    "\n",
    "ingestion_url = \"http://<HOST>:<PORT>\" # default is localhost:8001\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "config = pyaml_env.parse_config(\"../config/config.yaml\")\n",
    "\n",
    "# Initialize service\n",
    "init_data = {\"context_config\": config, \"uuid\": \"1\"}\n",
    "response = requests.post(\n",
    "    f\"{ingestion_url}/init\", headers=headers, data=json.dumps(init_data)\n",
    ")\n",
    "\n",
    "# Extract text documents from results\n",
    "add_doc_data_list = []\n",
    "doc_index = 0\n",
    "for _, doc in enumerate(results[0]):\n",
    "    if doc[\"document_type\"] == \"text\":\n",
    "        doc_data = {\n",
    "            \"document\": doc[\"metadata\"][\"content\"],\n",
    "            \"doc_index\": doc_index\n",
    "        }\n",
    "        \n",
    "        # Add metadata for first/last docs\n",
    "        if doc_index == 0:\n",
    "            doc_data[\"doc_metadata\"] = {\"is_first\": True}\n",
    "            \n",
    "        add_doc_data_list.append(doc_data)\n",
    "        doc_index += 1\n",
    "\n",
    "# Upload documents\n",
    "for add_doc_data in add_doc_data_list:\n",
    "    response = requests.post(\n",
    "        f\"{ingestion_url}/add_doc\", headers=headers, data=json.dumps(add_doc_data)\n",
    "    )\n",
    "    print(f\"Added document {add_doc_data['doc_index']}\")\n",
    "\n",
    "# Add the terminating document\n",
    "doc_data = {\n",
    "            \"document\": \".\",\n",
    "            \"doc_index\": doc_index,\n",
    "            \"doc_metadata\": {\"is_last\": True}\n",
    "        }\n",
    "response = requests.post(\n",
    "    f\"{ingestion_url}/add_doc\", headers=headers, data=json.dumps(doc_data)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Service\n",
    "In this section, we initialize the retrieval service with the same UUID used for data ingestion. We then send a request to summarize the document using the retrieval service's API endpoints. The service processes the question and returns a summary based on the previously ingested documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_url = \"http://<HOST>:<PORT>\" # default is http://localhost:8000\n",
    "\n",
    "# Initialize service with same uuid as data ingestion\n",
    "\n",
    "init_data = {\"context_config\": config, \"uuid\": \"1\"}\n",
    "\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{retrieval_url}/init\", headers=headers, data=json.dumps(init_data)\n",
    ")\n",
    "\n",
    "# Send retrieval request\n",
    "call_data = {\"retriever_function\": {\"question\": \"Summarize the document.\"}}\n",
    "request_data = {\"state\": call_data}\n",
    "response = requests.post(\n",
    "    f\"{retrieval_url}/call\", headers=headers, data=json.dumps(request_data)\n",
    ")\n",
    "print(response.json()[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
